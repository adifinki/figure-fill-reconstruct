{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe8fJ7sNdnul"
      },
      "source": [
        "# **Part II - Image Inpainting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM6EZSzEtwNA"
      },
      "source": [
        "The notebook includes all the code used for our project, and for each part we will explain the functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQhN-vEnVsOs"
      },
      "outputs": [],
      "source": [
        "#Before you run !!!\n",
        "\n",
        "#Make sure you have directory:\n",
        "#'/content/drive/MyDrive/FinalProject'\n",
        "\n",
        "#With the following files:\n",
        "#'/content/drive/MyDrive/FinalProject/model_architecture.png'\n",
        "#'/content/drive/MyDrive/FinalProject/states_tf_places2.pth'\n",
        "#'/content/drive/MyDrive/FinalProject/test/yuval_image.jpg'\n",
        "#'/content/drive/MyDrive/FinalProject/test/yuval_mask.png'\n",
        "#'/content/drive/MyDrive/FinalProject/original_images_team1'\n",
        "#'/content/drive/MyDrive/FinalProject/team1_mask'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGrEiTsHvEtX"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VWhbhUFvBE2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import skimage.transform\n",
        "import skimage.color\n",
        "import skimage.io as io\n",
        "from IPython.display import display\n",
        "from PIL import ImageOps\n",
        "from PIL import ImageDraw\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras.backend import clear_session\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "from skimage import io, transform\n",
        "\n",
        "plt.rcParams['figure.facecolor'] = 'white'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrH0Ny83PV82"
      },
      "source": [
        "## Inpainting model functions\n",
        "\n",
        "\n",
        "Based on the paper \"Free-Form Image Inpainting with Gated Convolution\": https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Free-Form_Image_Inpainting_With_Gated_Convolution_ICCV_2019_paper.pdf  <br> <br>\n",
        "The model is adapted from: https://github.com/nipponjo/deepfillv2-pytorch?tab=readme-ov-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VXAWXPDHxcNH"
      },
      "outputs": [],
      "source": [
        "#@title ##### Convolution\n",
        "\n",
        "def _init_conv_layer(conv, activation, mode='fan_out'):\n",
        "    \"\"\"\n",
        "    Initialize weights and biases of a convolutional layer based on the type of activation function used.\n",
        "\n",
        "    Parameters:\n",
        "    - conv (torch.nn.Module): The convolutional layer to initialize.\n",
        "    - activation (torch.nn.Module): The activation function used in the layer.\n",
        "    - mode (str, optional): The mode for weight initialization ('fan_out' or 'fan_in'). Default is 'fan_out'.\n",
        "    \"\"\"\n",
        "    if isinstance(activation, nn.LeakyReLU):\n",
        "        # Initialize weights using Kaiming initialization for LeakyReLU activation\n",
        "        torch.nn.init.kaiming_uniform_(conv.weight,\n",
        "                                       a=activation.negative_slope,\n",
        "                                       nonlinearity='leaky_relu',\n",
        "                                       mode=mode)\n",
        "    elif isinstance(activation, (nn.ReLU, nn.ELU)):\n",
        "        # Initialize weights using Kaiming initialization for ReLU or ELU activation\n",
        "        torch.nn.init.kaiming_uniform_(conv.weight,\n",
        "                                       nonlinearity='relu',\n",
        "                                       mode=mode)\n",
        "    else:\n",
        "        pass\n",
        "    if conv.bias is not None:\n",
        "        # Initialize biases to zeros if they exist\n",
        "        torch.nn.init.zeros_(conv.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N5hC64lc0U_U"
      },
      "outputs": [],
      "source": [
        "#@title ##### Output to Image\n",
        "\n",
        "def output_to_image(out):\n",
        "    \"\"\"\n",
        "    Convert tensor output to image format.\n",
        "\n",
        "    Parameters:\n",
        "    - out (torch.Tensor): Output tensor to convert to image.\n",
        "\n",
        "    Returns:\n",
        "    - out (numpy.ndarray): Image representation of the output tensor.\n",
        "    \"\"\"\n",
        "    # Move tensor to CPU, permute dimensions to bring channel dimension to the last axis, and adjust values to [0, 255] range\n",
        "    out = (out[0].cpu().permute(1, 2, 0) + 1.) * 127.5\n",
        "    # Convert tensor to numpy array of unsigned 8-bit integers\n",
        "    out = out.to(torch.uint8).numpy()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9c9ZKkmb0QHw"
      },
      "outputs": [],
      "source": [
        "#@title ##### Padding\n",
        "\n",
        "def same_padding(images, ksizes, strides, rates):\n",
        "    \"\"\"Implements tensorflow \"SAME\" padding as defined in:\n",
        "       https://github.com/tensorflow/tensorflow/blob/8eaf671025e8cd5358278f91f7e89e2fbbe6a26b/tensorflow/core/kernels/ops_util.cc#L65\n",
        "       see also: https://www.tensorflow.org/api_docs/python/tf/nn#same_padding_2\n",
        "\n",
        "       Calculates and applies the necessary padding to input images to ensure that\n",
        "       the output feature maps have the same spatial dimensions as the input images after convolution,\n",
        "       considering the specified kernel size, strides, and dilation rates.\n",
        "    \"\"\"\n",
        "    in_height, in_width = images.shape[2:]\n",
        "    out_height = -(in_height // -strides[0])  # ceil(a/b) = -(a//-b)\n",
        "    out_width = -(in_width // -strides[1])\n",
        "    filter_height = (ksizes[0]-1)*rates[0] + 1\n",
        "    filter_width = (ksizes[1]-1)*rates[1] + 1\n",
        "    pad_along_height = max(\n",
        "        (out_height-1)*strides[0] + filter_height - in_height, 0)\n",
        "    pad_along_width = max(\n",
        "        (out_width-1)*strides[1] + filter_width - in_width, 0)\n",
        "    pad_top = pad_along_height // 2\n",
        "    pad_bottom = pad_along_height - pad_top\n",
        "    pad_left = pad_along_width // 2\n",
        "    pad_right = pad_along_width - pad_left\n",
        "    paddings = (pad_left, pad_right, pad_top, pad_bottom)\n",
        "    padded_images = torch.nn.ZeroPad2d(paddings)(images)\n",
        "    return padded_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gw4LXV9y0QOk"
      },
      "outputs": [],
      "source": [
        "#@title ##### Downsampling\n",
        "def downsampling_nn_tf(images, n=2):\n",
        "    \"\"\"NN downsampling with tensorflow option align_corners=True \\\\\n",
        "       Args:\n",
        "           images: input\n",
        "           n: downsampling factor\n",
        "      performs downsampling of input images using nearest-neighbor interpolation,\n",
        "      where each output pixel value is determined by the nearest pixel in the input image.\n",
        "      This downsampling operation reduces the spatial dimensions of the input images by a\n",
        "      factor of n along both height and width dimensions.\n",
        "    \"\"\"\n",
        "    in_height, in_width = images.shape[2:]\n",
        "    out_height, out_width = in_height // n, in_width // n\n",
        "    height_inds = torch.linspace(0, in_height-1, steps=out_height, device=images.device).add_(0.5).floor_().long()\n",
        "    width_inds = torch.linspace(0, in_width-1, steps=out_width, device=images.device).add_(0.5).floor_().long()\n",
        "    return images[:, :, height_inds][..., width_inds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SHH9mrvoF2i0"
      },
      "outputs": [],
      "source": [
        "#@title ##### The Generator\n",
        "\n",
        "class GConv(nn.Module):\n",
        "    \"\"\"Implements the gated 2D convolution introduced in\n",
        "       `Free-Form Image Inpainting with Gated Convolution`(Yu et al., 2019) \\\\\n",
        "        Uses the SAME padding from tensorflow.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 cnum_in,\n",
        "                 cnum_out,\n",
        "                 ksize,\n",
        "                 stride=1,\n",
        "                 rate=1,\n",
        "                 padding='same',\n",
        "                 activation=nn.ELU()\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.cnum_out = cnum_out\n",
        "        num_conv_out = cnum_out if self.cnum_out == 3 or self.activation is None else 2*cnum_out\n",
        "        self.conv = nn.Conv2d(cnum_in,\n",
        "                              num_conv_out,\n",
        "                              kernel_size=ksize,\n",
        "                              stride=stride,\n",
        "                              padding=0,\n",
        "                              dilation=rate)\n",
        "\n",
        "        _init_conv_layer(self.conv, activation=self.activation)\n",
        "\n",
        "        self.ksize = ksize\n",
        "        self.stride = stride\n",
        "        self.rate = rate\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = same_padding(x, [self.ksize, self.ksize], [self.stride, self.stride],\n",
        "                         [self.rate, self.rate])\n",
        "\n",
        "        x = self.conv(x)\n",
        "        if self.cnum_out == 3 or self.activation is None:\n",
        "            return x\n",
        "        x, y = torch.split(x, self.cnum_out, dim=1)\n",
        "        x = self.activation(x)\n",
        "        y = torch.sigmoid(y)\n",
        "        x = x * y\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class GDeConv(nn.Module):\n",
        "    \"\"\"Upsampling followed by convolution\"\"\"\n",
        "\n",
        "    def __init__(self, cnum_in,\n",
        "                 cnum_out,\n",
        "                 padding=1):\n",
        "        super().__init__()\n",
        "        self.conv = GConv(cnum_in, cnum_out, 3, 1,\n",
        "                          padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest',\n",
        "                           recompute_scale_factor=False)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, cnum_in=5, cnum=48, return_flow=False, checkpoint=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # stage 1\n",
        "        self.conv1 = GConv(cnum_in, cnum//2, 5, 1, padding=2)\n",
        "\n",
        "        self.conv2_downsample = GConv(cnum//2, cnum, 3, 2)\n",
        "        self.conv3 = GConv(cnum, cnum, 3, 1)\n",
        "        self.conv4_downsample = GConv(cnum, 2*cnum, 3, 2)\n",
        "        self.conv5 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "\n",
        "        self.conv6 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "        self.conv7_atrous = GConv(2*cnum, 2*cnum, 3, rate=2, padding=2) #Atrous = Dilated\n",
        "        self.conv8_atrous = GConv(2*cnum, 2*cnum, 3, rate=4, padding=4)\n",
        "        self.conv9_atrous = GConv(2*cnum, 2*cnum, 3, rate=8, padding=8)\n",
        "        self.conv10_atrous = GConv(2*cnum, 2*cnum, 3, rate=16, padding=16)\n",
        "        self.conv11 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "        self.conv12 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "\n",
        "        self.conv13_upsample = GDeConv(2*cnum, cnum)\n",
        "        self.conv14 = GConv(cnum, cnum, 3, 1)\n",
        "        self.conv15_upsample = GDeConv(cnum, cnum//2)\n",
        "        self.conv16 = GConv(cnum//2, cnum//4, 3, 1)\n",
        "\n",
        "        self.conv17 = GConv(cnum//4, 3, 3, 1, activation=None)\n",
        "        self.tanh = nn.Tanh() #output between -1 and 1, suitable for image generation\n",
        "\n",
        "        # stage 2\n",
        "        # conv branch\n",
        "        self.xconv1 = GConv(3, cnum//2, 5, 1, padding=2)\n",
        "\n",
        "        self.xconv2_downsample = GConv(cnum//2, cnum//2, 3, 2)\n",
        "        self.xconv3 = GConv(cnum//2, cnum, 3, 1)\n",
        "        self.xconv4_downsample = GConv(cnum, cnum, 3, 2)\n",
        "        self.xconv5 = GConv(cnum, 2*cnum, 3, 1)\n",
        "\n",
        "        self.xconv6 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "        self.xconv7_atrous = GConv(2*cnum, 2*cnum, 3, rate=2, padding=2)\n",
        "        self.xconv8_atrous = GConv(2*cnum, 2*cnum, 3, rate=4, padding=4)\n",
        "        self.xconv9_atrous = GConv(2*cnum, 2*cnum, 3, rate=8, padding=8)\n",
        "        self.xconv10_atrous = GConv(2*cnum, 2*cnum, 3, rate=16, padding=16)\n",
        "\n",
        "        # attention branch\n",
        "        self.pmconv1 = GConv(3, cnum//2, 5, 1, padding=2)\n",
        "\n",
        "        self.pmconv2_downsample = GConv(cnum//2, cnum//2, 3, 2)\n",
        "        self.pmconv3 = GConv(cnum//2, cnum, 3, 1)\n",
        "        self.pmconv4_downsample = GConv(cnum, 2*cnum, 3, 2)\n",
        "        self.pmconv5 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "\n",
        "        self.pmconv6 = GConv(2*cnum, 2*cnum, 3, 1, activation=nn.ReLU())\n",
        "        self.contextual_attention = ContextualAttention(ksize=3,\n",
        "                                                        stride=1,\n",
        "                                                        rate=2,\n",
        "                                                        fuse_k=3,\n",
        "                                                        softmax_scale=10,\n",
        "                                                        fuse=False,\n",
        "                                                        device_ids=None,\n",
        "                                                        n_down=2,\n",
        "                                                        return_flow=return_flow)\n",
        "\n",
        "        self.pmconv9 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "        self.pmconv10 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "\n",
        "        self.allconv11 = GConv(4*cnum, 2*cnum, 3, 1)\n",
        "        self.allconv12 = GConv(2*cnum, 2*cnum, 3, 1)\n",
        "\n",
        "        self.allconv13_upsample = GDeConv(2*cnum, cnum)\n",
        "        self.allconv14 = GConv(cnum, cnum, 3, 1)\n",
        "        self.allconv15_upsample = GDeConv(cnum, cnum//2)\n",
        "        self.allconv16 = GConv(cnum//2, cnum//4, 3, 1)\n",
        "\n",
        "        self.allconv17 = GConv(cnum//4, 3, 3, 1, activation=None)\n",
        "\n",
        "        self.return_flow = return_flow\n",
        "\n",
        "        if checkpoint is not None:\n",
        "            generator_state_dict = torch.load(checkpoint)['G']\n",
        "            self.load_state_dict(generator_state_dict, strict=True)\n",
        "        self.eval();\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        xin = x\n",
        "\n",
        "        # stage 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2_downsample(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4_downsample(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7_atrous(x)\n",
        "        x = self.conv8_atrous(x)\n",
        "        x = self.conv9_atrous(x)\n",
        "        x = self.conv10_atrous(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.conv12(x)\n",
        "\n",
        "        x = self.conv13_upsample(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15_upsample(x)\n",
        "        x = self.conv16(x)\n",
        "\n",
        "        x = self.conv17(x)\n",
        "        x = self.tanh(x)\n",
        "        x_stage1 = x\n",
        "\n",
        "        # stage2, paste result as input\n",
        "        x = x*mask + xin[:, 0:3, :, :]*(1.-mask)\n",
        "\n",
        "        # conv branch\n",
        "        xnow = x\n",
        "        x = self.xconv1(xnow)\n",
        "        x = self.xconv2_downsample(x)\n",
        "        x = self.xconv3(x)\n",
        "        x = self.xconv4_downsample(x)\n",
        "        x = self.xconv5(x)\n",
        "\n",
        "        x = self.xconv6(x)\n",
        "        x = self.xconv7_atrous(x)\n",
        "        x = self.xconv8_atrous(x)\n",
        "        x = self.xconv9_atrous(x)\n",
        "        x = self.xconv10_atrous(x)\n",
        "        x_hallu = x\n",
        "\n",
        "        # attention branch\n",
        "        x = self.pmconv1(xnow)\n",
        "        x = self.pmconv2_downsample(x)\n",
        "        x = self.pmconv3(x)\n",
        "        x = self.pmconv4_downsample(x)\n",
        "        x = self.pmconv5(x)\n",
        "\n",
        "        x = self.pmconv6(x)\n",
        "        x, offset_flow = self.contextual_attention(x, x, mask)\n",
        "        x = self.pmconv9(x)\n",
        "        x = self.pmconv10(x)\n",
        "        pm = x\n",
        "        x = torch.cat([x_hallu, pm], dim=1)\n",
        "\n",
        "        x = self.allconv11(x)\n",
        "        x = self.allconv12(x)\n",
        "        x = self.allconv13_upsample(x)\n",
        "        x = self.allconv14(x)\n",
        "        x = self.allconv15_upsample(x)\n",
        "        x = self.allconv16(x)\n",
        "\n",
        "        x = self.allconv17(x)\n",
        "        x = self.tanh(x)\n",
        "        x_stage2 = x\n",
        "\n",
        "        if self.return_flow:\n",
        "            return x_stage1, x_stage2, offset_flow\n",
        "\n",
        "        return x_stage1, x_stage2\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def infer(self,\n",
        "              image,\n",
        "              mask,\n",
        "              return_vals=['inpainted', 'stage1'],\n",
        "              device='cuda'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image:\n",
        "            mask:\n",
        "            return_vals: inpainted, stage1, stage2, flow\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        _, h, w = image.shape\n",
        "        grid = 8\n",
        "\n",
        "        image = image[:3, :h//grid*grid, :w//grid*grid].unsqueeze(0)\n",
        "        mask = mask[0:1, :h//grid*grid, :w//grid*grid].unsqueeze(0)\n",
        "\n",
        "        image = (image*2 - 1.)  # map image values to [-1, 1] range\n",
        "        # 1.: masked 0.: unmasked\n",
        "        mask = (mask > 0.).to(dtype=torch.float32)\n",
        "\n",
        "        image_masked = image * (1.-mask)  # mask image\n",
        "\n",
        "        ones_x = torch.ones_like(image_masked)[:, 0:1, :, :]  # sketch channel\n",
        "        x = torch.cat([image_masked, ones_x, ones_x*mask],\n",
        "                      dim=1)  # concatenate channels\n",
        "\n",
        "        if self.return_flow:\n",
        "            x_stage1, x_stage2, offset_flow = self.forward(x, mask)\n",
        "        else:\n",
        "            x_stage1, x_stage2 = self.forward(x, mask)\n",
        "\n",
        "        image_compl = image * (1.-mask) + x_stage2 * mask\n",
        "\n",
        "        output = []\n",
        "        for return_val in return_vals:\n",
        "            if return_val.lower() == 'stage1':\n",
        "                output.append(output_to_image(x_stage1))\n",
        "            elif return_val.lower() == 'stage2':\n",
        "                output.append(output_to_image(x_stage2))\n",
        "            elif return_val.lower() == 'inpainted':\n",
        "                output.append(output_to_image(image_compl))\n",
        "            elif return_val.lower() == 'flow' and self.return_flow:\n",
        "                output.append(offset_flow)\n",
        "            else:\n",
        "                print(f'Invalid return value: {return_val}')\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QF3haODwxSN2"
      },
      "outputs": [],
      "source": [
        "#@title ##### Contextual Attention\n",
        "\n",
        "\"\"\"\n",
        "adapted from: https://github.com/daa233/generative-inpainting-pytorch/blob/c6cdaea0427b37b5b38a3f48d4355abf9566c659/model/networks.py\n",
        "\"\"\"\n",
        "class ContextualAttention(nn.Module):\n",
        "    \"\"\" Contextual attention layer implementation. \\\\\n",
        "        Contextual attention is first introduced in publication: \\\\\n",
        "        `Generative Image Inpainting with Contextual Attention`(Yu et al., 2019) \\\\\n",
        "        Args:\n",
        "            ksize: Kernel size for contextual attention\n",
        "            stride: Stride for extracting patches from b\n",
        "            rate: Dilation for matching\n",
        "            softmax_scale: Scaled softmax for attention\n",
        "\n",
        "        Captures contextual information between foreground and background features,\n",
        "        allowing the model to focus on relevant regions during inpainting tasks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 ksize=3,\n",
        "                 stride=1,\n",
        "                 rate=1,\n",
        "                 fuse_k=3,\n",
        "                 softmax_scale=10.,\n",
        "                 n_down=2,\n",
        "                 fuse=True,\n",
        "                 return_flow=False,\n",
        "                 device_ids=None):\n",
        "        super(ContextualAttention, self).__init__()\n",
        "        self.ksize = ksize\n",
        "        self.stride = stride\n",
        "        self.rate = rate\n",
        "        self.fuse_k = fuse_k\n",
        "        self.softmax_scale = softmax_scale\n",
        "        self.fuse = fuse\n",
        "        self.device_ids = device_ids\n",
        "        self.n_down = n_down\n",
        "        self.return_flow = return_flow\n",
        "\n",
        "    def forward(self, f, b, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            f: Input feature to match (foreground).\n",
        "            b: Input feature for match (background).\n",
        "            mask: Input mask for b, indicating patches not available.\n",
        "        \"\"\"\n",
        "        device = f.device\n",
        "        # get shapes\n",
        "        raw_int_fs, raw_int_bs = list(f.size()), list(b.size())   # b*c*h*w\n",
        "\n",
        "        # extract patches from background with stride and rate\n",
        "        kernel = 2 * self.rate\n",
        "        # raw_w is extracted for reconstruction\n",
        "        raw_w = extract_image_patches(b, ksizes=[kernel, kernel],\n",
        "                                         strides=[self.rate*self.stride,\n",
        "                                                  self.rate*self.stride],\n",
        "                                         rates=[1, 1], padding='same')  # [N, C*k*k, L]\n",
        "        # raw_shape: [N, C, k, k, L]\n",
        "        raw_w = raw_w.view(raw_int_bs[0], raw_int_bs[1], kernel, kernel, -1)\n",
        "        raw_w = raw_w.permute(0, 4, 1, 2, 3)    # raw_shape: [N, L, C, k, k]\n",
        "        raw_w_groups = torch.split(raw_w, 1, dim=0)\n",
        "\n",
        "        # downscaling foreground option: downscaling both foreground and\n",
        "        # background for matching and use original background for reconstruction.\n",
        "        f = downsampling_nn_tf(f, n=self.rate)\n",
        "        b = downsampling_nn_tf(b, n=self.rate)\n",
        "        int_fs, int_bs = list(f.size()), list(b.size())   # b*c*h*w\n",
        "        # split tensors along the batch dimension\n",
        "        f_groups = torch.split(f, 1, dim=0)\n",
        "        # w shape: [N, C*k*k, L]\n",
        "        w = extract_image_patches(b, ksizes=[self.ksize, self.ksize],\n",
        "                                     strides=[self.stride, self.stride],\n",
        "                                     rates=[1, 1], padding='same')\n",
        "        # w shape: [N, C, k, k, L]\n",
        "        w = w.view(int_bs[0], int_bs[1], self.ksize, self.ksize, -1)\n",
        "        w = w.permute(0, 4, 1, 2, 3)    # w shape: [N, L, C, k, k]\n",
        "        w_groups = torch.split(w, 1, dim=0)\n",
        "\n",
        "        # process mask\n",
        "        if mask is None:\n",
        "            mask = torch.zeros([int_bs[0], 1, int_bs[2], int_bs[3]], device=device)\n",
        "        else:\n",
        "            mask = downsampling_nn_tf(mask, n=(2**self.n_down)*self.rate)\n",
        "        int_ms = list(mask.size())\n",
        "        # m shape: [N, C*k*k, L]\n",
        "        m = extract_image_patches(mask, ksizes=[self.ksize, self.ksize],\n",
        "                                        strides=[self.stride, self.stride],\n",
        "                                        rates=[1, 1], padding='same')\n",
        "        # m shape: [N, C, k, k, L]\n",
        "        m = m.view(int_ms[0], int_ms[1], self.ksize, self.ksize, -1)\n",
        "        m = m.permute(0, 4, 1, 2, 3)    # m shape: [N, L, C, k, k]\n",
        "        m = m[0]    # m shape: [L, C, k, k]\n",
        "\n",
        "        # mm shape: [L, 1, 1, 1]\n",
        "        mm = (torch.mean(m, axis=[1, 2, 3], keepdim=True) == 0.).to(torch.float32)\n",
        "        mm = mm.permute(1, 0, 2, 3)  # mm shape: [1, L, 1, 1]\n",
        "\n",
        "        y = []\n",
        "        offsets = []\n",
        "        k = self.fuse_k\n",
        "        scale = self.softmax_scale    # to fit the PyTorch tensor image value range\n",
        "        fuse_weight = torch.eye(k, device=device).view(1, 1, k, k)  # 1*1*k*k\n",
        "\n",
        "        for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n",
        "            '''\n",
        "            O => output channel as a conv filter\n",
        "            I => input channel as a conv filter\n",
        "            xi : separated tensor along batch dimension of front; (B=1, C=128, H=32, W=32)\n",
        "            wi : separated patch tensor along batch dimension of back; (B=1, O=32*32, I=128, KH=3, KW=3)\n",
        "            raw_wi : separated tensor along batch dimension of back; (B=1, I=32*32, O=128, KH=4, KW=4)\n",
        "            '''\n",
        "            # conv for compare\n",
        "            wi = wi[0]  # [L, C, k, k]\n",
        "            max_wi = torch.sqrt(torch.sum(torch.pow(wi, 2), dim=[1, 2, 3], keepdim=True)).clamp_min(1e-4)\n",
        "            wi_normed = wi / max_wi\n",
        "            # xi shape: [1, C, H, W], yi shape: [1, L, H, W]\n",
        "            xi = same_padding(xi, [self.ksize, self.ksize], [1, 1], [1, 1])  # xi: 1*c*H*W\n",
        "            yi = F.conv2d(xi, wi_normed, stride=1)   # [1, L, H, W]\n",
        "            # conv implementation for fuse scores to encourage large patches\n",
        "            if self.fuse:\n",
        "                # make all of depth to spatial resolution\n",
        "                # (B=1, I=1, H=32*32, W=32*32)\n",
        "                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n",
        "                # (B=1, C=1, H=32*32, W=32*32)\n",
        "                yi = same_padding(yi, [k, k], [1, 1], [1, 1])\n",
        "                yi = F.conv2d(yi, fuse_weight, stride=1)\n",
        "                # (B=1, 32, 32, 32, 32)\n",
        "                yi = yi.contiguous().view(1, int_bs[2], int_bs[3], int_fs[2], int_fs[3])\n",
        "                yi = yi.permute(0, 2, 1, 4, 3).contiguous()\n",
        "\n",
        "                yi = yi.view(1, 1, int_bs[2]*int_bs[3], int_fs[2]*int_fs[3])\n",
        "                yi = same_padding(yi, [k, k], [1, 1], [1, 1])\n",
        "                yi = F.conv2d(yi, fuse_weight, stride=1)\n",
        "                yi = yi.contiguous().view(1, int_bs[3], int_bs[2], int_fs[3], int_fs[2])\n",
        "                yi = yi.permute(0, 2, 1, 4, 3).contiguous()\n",
        "            # (B=1, C=32*32, H=32, W=32)\n",
        "            yi = yi.view(1, int_bs[2] * int_bs[3], int_fs[2], int_fs[3])\n",
        "\n",
        "            # softmax to match\n",
        "            yi = yi * mm\n",
        "            yi = F.softmax(yi*scale, dim=1)\n",
        "            yi = yi * mm  # [1, L, H, W]\n",
        "\n",
        "            if self.return_flow:\n",
        "                offset = torch.argmax(yi, dim=1, keepdim=True)  # 1*1*H*W\n",
        "\n",
        "                if int_bs != int_fs:\n",
        "                    # Normalize the offset value to match foreground dimension\n",
        "                    times = (int_fs[2]*int_fs[3])/(int_bs[2]*int_bs[3])\n",
        "                    offset = ((offset + 1) * times - 1).to(torch.int64)\n",
        "                offset = torch.cat([torch.div(offset, int_fs[3], rounding_mode='trunc'),\n",
        "                                    offset % int_fs[3]], dim=1)  # 1*2*H*W\n",
        "\n",
        "                offsets.append(offset)\n",
        "\n",
        "            # deconv for patch pasting\n",
        "            wi_center = raw_wi[0]\n",
        "            yi = F.conv_transpose2d(yi, wi_center, stride=self.rate, padding=1) / 4.  # (B=1, C=128, H=64, W=64)\n",
        "            y.append(yi)\n",
        "\n",
        "        y = torch.cat(y, dim=0)  # back to the mini-batch\n",
        "        y = y.contiguous().view(raw_int_fs)\n",
        "\n",
        "        if not self.return_flow:\n",
        "            return y, None\n",
        "\n",
        "        offsets = torch.cat(offsets, dim=0)\n",
        "        offsets = offsets.view(int_fs[0], 2, *int_fs[2:])\n",
        "\n",
        "        # case1: visualize optical flow: minus current position\n",
        "        h_add = torch.arange(int_fs[2], device=device).view([1, 1, int_fs[2], 1]).expand(int_fs[0], -1, -1, int_fs[3])\n",
        "        w_add = torch.arange(int_fs[3], device=device).view([1, 1, 1, int_fs[3]]).expand(int_fs[0], -1, int_fs[2], -1)\n",
        "        offsets = offsets - torch.cat([h_add, w_add], dim=1)\n",
        "        # to flow image\n",
        "        flow = torch.from_numpy(flow_to_image(offsets.permute(0, 2, 3, 1).detach().cpu().numpy())) / 255.\n",
        "        flow = flow.permute(0, 3, 1, 2)\n",
        "        # case2: visualize which pixels are attended\n",
        "        # flow = torch.from_numpy(highlight_flow((offsets * mask.long()).detach().cpu().numpy()))\n",
        "\n",
        "        if self.rate != 1:\n",
        "            flow = F.interpolate(flow, scale_factor=self.rate, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return y, flow\n",
        "\n",
        "#----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mHI7K1Y0vDKL"
      },
      "outputs": [],
      "source": [
        "#@title ##### Extracting and Coloring\n",
        "\n",
        "def extract_image_patches(images, ksizes, strides, rates, padding='same'):\n",
        "    \"\"\"\n",
        "    Extracts sliding local blocks \\\\\n",
        "    see also: https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html\n",
        "    Patching - breaking down a large picture into more managable units\n",
        "    \"\"\"\n",
        "\n",
        "    if padding == 'same':\n",
        "        images = same_padding(images, ksizes, strides, rates)\n",
        "        padding = 0\n",
        "\n",
        "    unfold = torch.nn.Unfold(kernel_size=ksizes,\n",
        "                             stride=strides,\n",
        "                             padding=padding,\n",
        "                             dilation=rates\n",
        "                             )\n",
        "    patches = unfold(images)\n",
        "    return patches  # [N, C*k*k, L], L is the total number of such blocks\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def flow_to_image(flow):\n",
        "    \"\"\"Transfer flow map to image.\n",
        "    Part of code forked from flownet.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    maxu = -999.\n",
        "    maxv = -999.\n",
        "    minu = 999.\n",
        "    minv = 999.\n",
        "    maxrad = -1\n",
        "    for i in range(flow.shape[0]):\n",
        "        u = flow[i, :, :, 0]\n",
        "        v = flow[i, :, :, 1]\n",
        "        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)\n",
        "        u[idxunknow] = 0\n",
        "        v[idxunknow] = 0\n",
        "        maxu = max(maxu, np.max(u))\n",
        "        minu = min(minu, np.min(u))\n",
        "        maxv = max(maxv, np.max(v))\n",
        "        minv = min(minv, np.min(v))\n",
        "        rad = np.sqrt(u ** 2 + v ** 2)\n",
        "        maxrad = max(maxrad, np.max(rad))\n",
        "        u = u / (maxrad + np.finfo(float).eps)\n",
        "        v = v / (maxrad + np.finfo(float).eps)\n",
        "        img = compute_color(u, v)\n",
        "        out.append(img)\n",
        "    return np.float32(np.uint8(out))\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def compute_color(u, v):\n",
        "    h, w = u.shape\n",
        "    img = np.zeros([h, w, 3])\n",
        "    nanIdx = np.isnan(u) | np.isnan(v)\n",
        "    u[nanIdx] = 0\n",
        "    v[nanIdx] = 0\n",
        "    # colorwheel = COLORWHEEL\n",
        "    colorwheel = make_color_wheel()\n",
        "    ncols = np.size(colorwheel, 0)\n",
        "    rad = np.sqrt(u ** 2 + v ** 2)\n",
        "    a = np.arctan2(-v, -u) / np.pi\n",
        "    fk = (a + 1) / 2 * (ncols - 1) + 1\n",
        "    k0 = np.floor(fk).astype(int)\n",
        "    k1 = k0 + 1\n",
        "    k1[k1 == ncols + 1] = 1\n",
        "    f = fk - k0\n",
        "    for i in range(np.size(colorwheel, 1)):\n",
        "        tmp = colorwheel[:, i]\n",
        "        col0 = tmp[k0 - 1] / 255\n",
        "        col1 = tmp[k1 - 1] / 255\n",
        "        col = (1 - f) * col0 + f * col1\n",
        "        idx = rad <= 1\n",
        "        col[idx] = 1 - rad[idx] * (1 - col[idx])\n",
        "        notidx = np.logical_not(idx)\n",
        "        col[notidx] *= 0.75\n",
        "        img[:, :, i] = np.uint8(np.floor(255 * col * (1 - nanIdx)))\n",
        "    return img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def make_color_wheel():\n",
        "    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n",
        "    ncols = RY + YG + GC + CB + BM + MR\n",
        "    colorwheel = np.zeros([ncols, 3])\n",
        "    col = 0\n",
        "    # RY\n",
        "    colorwheel[0:RY, 0] = 255\n",
        "    colorwheel[0:RY, 1] = np.transpose(np.floor(255 * np.arange(0, RY) / RY))\n",
        "    col += RY\n",
        "    # YG\n",
        "    colorwheel[col:col + YG, 0] = 255 - \\\n",
        "        np.transpose(np.floor(255 * np.arange(0, YG) / YG))\n",
        "    colorwheel[col:col + YG, 1] = 255\n",
        "    col += YG\n",
        "    # GC\n",
        "    colorwheel[col:col + GC, 1] = 255\n",
        "    colorwheel[col:col + GC,\n",
        "               2] = np.transpose(np.floor(255 * np.arange(0, GC) / GC))\n",
        "    col += GC\n",
        "    # CB\n",
        "    colorwheel[col:col + CB, 1] = 255 - \\\n",
        "        np.transpose(np.floor(255 * np.arange(0, CB) / CB))\n",
        "    colorwheel[col:col + CB, 2] = 255\n",
        "    col += CB\n",
        "    # BM\n",
        "    colorwheel[col:col + BM, 2] = 255\n",
        "    colorwheel[col:col + BM,\n",
        "               0] = np.transpose(np.floor(255 * np.arange(0, BM) / BM))\n",
        "    col += + BM\n",
        "    # MR\n",
        "    colorwheel[col:col + MR, 2] = 255 - \\\n",
        "        np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
        "    colorwheel[col:col + MR, 0] = 255\n",
        "    return colorwheel\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQ-IrwqannD"
      },
      "source": [
        "## Mount the drive & Import model's weights\n",
        "make sure you have all the files listed in the begining of the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTRKCMROGAUo",
        "outputId": "b8067b3a-e4ff-4e82-9038-54142f2b5a73"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImPIT52yeDkL"
      },
      "source": [
        "#### Load generator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frkyi4DKd_-T"
      },
      "outputs": [],
      "source": [
        "use_cuda_if_available = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available()\n",
        "                             and use_cuda_if_available else 'cpu')\n",
        "\n",
        "sd_path = '/content/drive/MyDrive/FinalProject/states_tf_places2.pth'\n",
        "\n",
        "generator = Generator(checkpoint=sd_path, return_flow=True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbkHlmP7mJh7"
      },
      "source": [
        "## Inpainting pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4jneevWgpw0"
      },
      "outputs": [],
      "source": [
        "def inpainting_pipe_by_image(image_path, mask_path, output_path, team1 = True):\n",
        "    for image_name in os.listdir(image_path):\n",
        "        try:\n",
        "            # Open image and corresponding mask\n",
        "            image_pil = Image.open(os.path.join(image_path, image_name))\n",
        "            if team1:\n",
        "              mask_pil = Image.open(os.path.join(mask_path, image_name))\n",
        "            else:\n",
        "              mask_pil = Image.open(os.path.join(mask_path, \"mask_\" + image_name))\n",
        "\n",
        "            # Process images\n",
        "            image = T.ToTensor()(image_pil).to(device)\n",
        "            mask = T.ToTensor()(mask_pil).to(device)\n",
        "            output = generator.infer(image, mask, return_vals=['inpainted', 'stage1', 'stage2', 'flow'])\n",
        "\n",
        "            # Convert numpy array to PIL image to save\n",
        "            output_image = Image.fromarray(output[0])\n",
        "            output_image.save(os.path.join(output_path, image_name))\n",
        "        except RuntimeError as e:\n",
        "            print(f\"RuntimeError occurred processing image {image_name}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred processing image {image_name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Q2_jG_5Ix_"
      },
      "source": [
        "## Mask modifitcation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF9O6PO25qRW"
      },
      "outputs": [],
      "source": [
        "def adjust_rgba_mask(path):\n",
        "    mask = Image.open(path)\n",
        "    pixel_data = list(mask.getdata())\n",
        "\n",
        "    # Create a new image with the same size and mode as the original image\n",
        "    adjusted_im = Image.new('RGBA', mask.size)\n",
        "\n",
        "\n",
        "    # Iterate over each pixel\n",
        "    for i, pixel in enumerate(pixel_data):\n",
        "        # Extract intensity value (since it's grayscale)\n",
        "        intensity = pixel\n",
        "\n",
        "        # Determine alpha value based on intensity\n",
        "        if intensity == 0:\n",
        "            alpha = 0\n",
        "        else:\n",
        "            alpha = 255\n",
        "\n",
        "        # Set the pixel value with the determined alpha value\n",
        "        new_pixel = (intensity,intensity,intensity, alpha)\n",
        "        adjusted_im.putpixel((i % mask.width, i // mask.width), new_pixel)\n",
        "\n",
        "    return adjusted_im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue_bAOsOeRT9"
      },
      "outputs": [],
      "source": [
        "def adjust_rgba_mask_team1(path):\n",
        "\n",
        "  mask = Image.open(path)\n",
        "  pixel_data = list(mask.getdata())\n",
        "\n",
        "  # Create a new image with the same size and mode as the original image\n",
        "  adjusted_im = Image.new(mask.mode, mask.size)\n",
        "\n",
        "  # Iterate over each pixel\n",
        "  for i, pixel in enumerate(pixel_data):\n",
        "    # Extract RGB values\n",
        "    r, g, b = pixel[:3]\n",
        "\n",
        "    # Modify RGB values\n",
        "    if (r, g, b) == (68, 1, 84):\n",
        "      r, g, b = 0, 0, 0\n",
        "    elif (r, g, b) == (253, 231, 36):\n",
        "      r, g, b = 255, 255, 255\n",
        "\n",
        "    # Determine alpha value based on modified RGB\n",
        "    if (r, g, b) == (0, 0, 0):\n",
        "      alpha = 0\n",
        "    else:\n",
        "      alpha = 255\n",
        "\n",
        "    # Set the pixel value with the determined alpha value\n",
        "    new_pixel = (r, g, b, alpha)\n",
        "    adjusted_im.putpixel((i % mask.width, i // mask.width), new_pixel)\n",
        "\n",
        "  return adjusted_im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-DEHuYX93Ok"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def blobify(mask, radius):\n",
        "    # Create a new image with the same size and mode as the original mask image\n",
        "    blob_mask = Image.new('RGBA', mask.size)\n",
        "\n",
        "    # Create a drawing context\n",
        "    draw = ImageDraw.Draw(blob_mask)\n",
        "\n",
        "    # Iterate over each pixel in the mask image\n",
        "    for y in range(mask.size[1]):\n",
        "        for x in range(mask.size[0]):\n",
        "            # Get the pixel value at the current coordinates\n",
        "            pixel = mask.getpixel((x, y))\n",
        "\n",
        "            # Check if the pixel is part of the cluster (white)\n",
        "            if pixel == (255, 255, 255, 255):\n",
        "                # Draw a rounded rectangle centered around the pixel\n",
        "                x1, y1 = x - radius, y - radius\n",
        "                x2, y2 = x + radius, y + radius\n",
        "                draw.ellipse([(x1, y1), (x2, y2)], fill=(255, 255, 255, 255))\n",
        "\n",
        "    # Merge the modified image with the rounded rectangles\n",
        "    blob_mask = Image.alpha_composite(mask.convert('RGBA'), blob_mask)\n",
        "\n",
        "    # Delete the drawing context\n",
        "    del draw\n",
        "\n",
        "    return blob_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNrFTWuDmLWE"
      },
      "source": [
        "## Mask modification pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InWGHe_4SPyS"
      },
      "outputs": [],
      "source": [
        "def modification_pipe(path_input, dir_output, radius, team1):\n",
        "    if os.path.isdir(path_input):\n",
        "        for filename in os.listdir(path_input):\n",
        "            file_path = os.path.join(path_input, filename)\n",
        "            if os.path.isfile(file_path):\n",
        "                try:\n",
        "                    # Choose the appropriate adjust_rgba_mask function based on the 'team1' flag\n",
        "                    if team1:\n",
        "                        adj_mask = adjust_rgba_mask_team1(file_path)\n",
        "                    else:\n",
        "                        adj_mask = adjust_rgba_mask(file_path)\n",
        "\n",
        "                    # Apply blobification\n",
        "                    mod_mask = blobify(adj_mask, radius)\n",
        "\n",
        "                    # Save the modified mask to the output directory\n",
        "                    output_file_path = os.path.join(dir_output, filename)\n",
        "                    mod_mask.save(output_file_path)\n",
        "                    print(f\"Saved modified mask to {output_file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"{file_path} is not a file. Skipping...\")\n",
        "    else:\n",
        "        print(\"The input path does not exist or is not a directory.\")\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr8WQoNL3max"
      },
      "source": [
        "# Inpainting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLm7h1hr3vH4"
      },
      "source": [
        "We will utilize our inpainting model to process the same image twice. Firstly, we will employ the official masks for inpainting, followed by the masks generated by team #1. This approach allows us to evaluate the model's efficiency and performance without being reliant on the quality of the mask. By comparing the results obtained from both sets of masks, we can gain valuable insights into the effectiveness of the inpainting model under different conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKl4b8333Z7O"
      },
      "source": [
        "### Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAkeziiImQX8"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "image_path = '/content/drive/MyDrive/FinalProject/original_images'\n",
        "image_path_team1 = '/content/drive/MyDrive/FinalProject/original_images_team1'\n",
        "mask_path = '/content/drive/MyDrive/FinalProject/modified_masks'\n",
        "mask_path_team1 = '/content/drive/MyDrive/FinalProject/modified_masks_team1'\n",
        "output_path = '/content/drive/MyDrive/FinalProject/inpainted_images'\n",
        "output_path_team1 = '/content/drive/MyDrive/FinalProject/inpainted_images_team1'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(image_path, exist_ok=True)\n",
        "os.makedirs(image_path_team1, exist_ok=True)\n",
        "os.makedirs(mask_path, exist_ok=True)\n",
        "os.makedirs(mask_path_team1, exist_ok=True)\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(output_path_team1, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McoM5lvzEV7V"
      },
      "outputs": [],
      "source": [
        "os.makedirs(mask_path_team1, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r384yi2hxuVS"
      },
      "source": [
        "## Use pipline on COCO dataset test set with the official mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3mJ9eJNx_V5"
      },
      "source": [
        "### Download the test set that was used in part 1, and use the offficial mask for inpainting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeT4GHUfx-fH"
      },
      "outputs": [],
      "source": [
        "MAX_NUM_IMAGES = 2560\n",
        "BASE_DIR = '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwEp9-S_kw99"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f\"{BASE_DIR}/annotations_trainval2017.zip\"):\n",
        "  !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "  !unzip -o annotations_trainval2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1pNOXHUp48A"
      },
      "outputs": [],
      "source": [
        "trainDataType = 'train2017'\n",
        "\n",
        "trainAnnFile = f'{BASE_DIR}/annotations/instances_{trainDataType}.json'\n",
        "\n",
        "STORED_IMAGE_DIR = f'{BASE_DIR}/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSLM8fpdp-sj"
      },
      "outputs": [],
      "source": [
        "coco=COCO(trainAnnFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE6A0VaoqVte"
      },
      "outputs": [],
      "source": [
        "cats = coco.loadCats(coco.getCatIds())\n",
        "nms=[cat['name'] for cat in cats]\n",
        "print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
        "\n",
        "nms = set([cat['supercategory'] for cat in cats])\n",
        "print('COCO supercategories: \\n{}'.format(' '.join(nms)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "nqdPT0KqqdaX",
        "outputId": "3784af6e-a313-40e3-92f2-6068e648da1e"
      },
      "outputs": [],
      "source": [
        "CATEGORY_IDS = coco.getCatIds(catNms=['person'])\n",
        "_image_ids = list(set(coco.getImgIds(catIds=CATEGORY_IDS)))  # Make sure there are no duplications\n",
        "UNUSED_IMAGE_IDS = _image_ids[MAX_NUM_IMAGES:]\n",
        "IMAGE_IDS = _image_ids[2304:2560] # These are the images used for test from team #1 model\n",
        "IMAGE_SIZE = (512, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULeOgRBG3uHr"
      },
      "outputs": [],
      "source": [
        "target_dir = \"/content/drive/My Drive/FinalProject/COCO_dataset\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "mask_target_dir = \"/content/drive/My Drive/FinalProject/COCO_dataset_mask\"\n",
        "os.makedirs(mask_target_dir, exist_ok=True)  # Create the target directory if it doesn't exist\n",
        "\n",
        "image_target_dir = image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxaodv9hNXSJ"
      },
      "outputs": [],
      "source": [
        "def get_mask(images_ids: list[int], category_ids: list[int], MIN_MASK_CAPACITY_PERCENTAGE: float = 0.1, IMAGE_SIZE: tuple = (224, 224)):\n",
        "    # Create list to store image IDs that don't meet mask capacity percentage\n",
        "    dont_use = []\n",
        "    i = 0\n",
        "    # Loop through each image ID\n",
        "    for image_id in images_ids:\n",
        "        #download image\n",
        "        coco.download(tarDir=target_dir, imgIds=[image_id])\n",
        "        image_path = f\"{target_dir}/{image_id:012d}.jpg\"\n",
        "\n",
        "        # Load annotations for the specified image ID and category IDs\n",
        "        annotations = coco.loadAnns(coco.getAnnIds(imgIds=image_id, catIds=category_ids))\n",
        "\n",
        "        # Load the corresponding image\n",
        "        image = io.imread(image_path)\n",
        "        n_image = image / 255.  # Normalize image\n",
        "\n",
        "        # Create an empty mask\n",
        "        mask = np.zeros((n_image.shape[0], n_image.shape[1]), dtype=np.uint8)\n",
        "\n",
        "        # Draw segmentation masks on the empty mask\n",
        "        for annotation in annotations:\n",
        "            segmentation_mask = coco.annToMask(annotation)\n",
        "            if segmentation_mask.shape != mask.shape:\n",
        "                # Resize the segmentation mask to match the shape of the empty mask\n",
        "                segmentation_mask = cv2.resize(segmentation_mask, (mask.shape[1], mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "            mask += segmentation_mask\n",
        "\n",
        "        # Apply binary thresholding to the mask\n",
        "        mask[mask >= 1] = 255\n",
        "\n",
        "        # Check if the mask meets the minimum capacity percentage\n",
        "        mask_percentage = 0.05\n",
        "        if mask.sum() <= mask.size * mask_percentage:\n",
        "            # If not, add the image ID to the list of images not to use\n",
        "            dont_use.append(image_id)\n",
        "\n",
        "        # Resize image and mask\n",
        "        image = transform.resize(image, IMAGE_SIZE, preserve_range=True, anti_aliasing=False).astype(np.uint8)\n",
        "        mask = transform.resize(mask, IMAGE_SIZE, preserve_range=True, anti_aliasing=False)\n",
        "\n",
        "        # Save the segmentation mask to the target directory\n",
        "        mask_filename = f\"mask_image_{i}.png\"\n",
        "        mask_path = os.path.join(mask_target_dir, mask_filename)\n",
        "        cv2.imwrite(mask_path, mask)\n",
        "\n",
        "        # Save the resized image to the target directory\n",
        "        resized_image_filename = f\"image_{i}.png\"  # Adjust the filename as needed\n",
        "        resized_image_path = os.path.join(image_target_dir, resized_image_filename)\n",
        "        io.imsave(resized_image_path, image)  # Save the resized image\n",
        "\n",
        "        # Display the image and segmentation mask\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "        axes[0].imshow(image)\n",
        "        axes[0].set_title(\"Original Image\")\n",
        "        axes[0].axis(\"off\")\n",
        "        axes[1].imshow(mask, cmap='gray')\n",
        "        axes[1].set_title(\"Segmentation Mask\")\n",
        "        axes[1].axis(\"off\")\n",
        "        plt.show()\n",
        "        i +=1\n",
        "\n",
        "    return dont_use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0RseQwaBjIg"
      },
      "outputs": [],
      "source": [
        "#a = get_mask(IMAGE_IDS, CATEGORY_IDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS2WGYfE0uRI"
      },
      "source": [
        "## Modify masks to fit the model format and to improve results by Blubifying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0z3BmD0nF1pI",
        "outputId": "4743093a-9ba9-4853-87f7-be4c997e1d3d"
      },
      "outputs": [],
      "source": [
        "mask_target_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl1yT3d4VE4C"
      },
      "outputs": [],
      "source": [
        "# For official masks\n",
        "modification_pipe(mask_target_dir, mask_path, 10, team1 = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXEtpbbcEk-d"
      },
      "outputs": [],
      "source": [
        "# For team #1 masks\n",
        "# Make sure you upload this file ['/content/drive/MyDrive/FinalProject/team1_mask'] to the directory before running\n",
        "modification_pipe('/content/drive/MyDrive/FinalProject/team1_masks', mask_path_team1, 10, team1 = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uVPFbRyy7yT"
      },
      "source": [
        "## Inpaint the Test set with the offical masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KcuHn5UC30H"
      },
      "outputs": [],
      "source": [
        "def plot_image_and_mask(image_path, mask_path):\n",
        "    # Open image and mask\n",
        "    image = Image.open(image_path)\n",
        "    mask = Image.open(mask_path)\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Plot image on the first subplot\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Plot mask on the second subplot\n",
        "    axes[1].imshow(mask, cmap='gray')\n",
        "    axes[1].set_title('Mask')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Adjust layout and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VcwU3oScC3gh",
        "outputId": "01cce8b5-b830-4052-f580-d90b680b7805"
      },
      "outputs": [],
      "source": [
        "'''def plot_images_with_masks(mask_dir, output_dir):\n",
        "    # Get list of image files in mask directory\n",
        "    image_files = os.listdir(mask_dir)\n",
        "\n",
        "    # Iterate over each image file\n",
        "    for image_file in image_files:\n",
        "        # Construct paths for mask and output image\n",
        "        mask_path = os.path.join(mask_dir, image_file)\n",
        "        output_path = os.path.join(output_dir, image_file)\n",
        "\n",
        "        # Plot image and mask side by side\n",
        "        plot_image_and_mask(mask_path, output_path)\n",
        "\n",
        "# Call the function for the specified directories\n",
        "mask_path_team1 = '/content/drive/MyDrive/FinalProject/modified_masks_team1'\n",
        "output_path_team1 = '/content/drive/MyDrive/FinalProject/original_images_team1'\n",
        "plot_images_with_masks(mask_path_team1, output_path_team1)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz5EwiMFy5Zf",
        "outputId": "6ca39d8f-be26-4fa3-ed43-027571533fc6"
      },
      "outputs": [],
      "source": [
        "inpainting_pipe_by_image(image_path, mask_path, output_path,team1 = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSG90W8B0rQC"
      },
      "source": [
        "### inpaint on the test set with team #1 masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wYSijYF0pvV"
      },
      "outputs": [],
      "source": [
        "inpainting_pipe_by_image(image_path_team1, mask_path_team1, output_path_team1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl4zW4PB2yba"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X7y-2CGU24Hl",
        "outputId": "2a10f2a0-926f-4550-b1ba-849d7a7bda9b"
      },
      "outputs": [],
      "source": [
        "def plot_images_side_by_side(image_paths, titles):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Create a figure with three subplots\n",
        "\n",
        "    for i, (image_path, title) in enumerate(zip(image_paths, titles)):\n",
        "        try:\n",
        "            # Load and display the image\n",
        "            image = Image.open(image_path)\n",
        "            axes[i].imshow(image)\n",
        "            axes[i].set_title(title)\n",
        "            axes[i].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "folder1_path = image_path\n",
        "folder2_path = output_path_team1\n",
        "folder3_path = output_path\n",
        "\n",
        "# Get list of image files in each folder\n",
        "for filename in os.listdir(folder1_path):\n",
        "    try:\n",
        "        # Select three images from each folder for plotting\n",
        "        selected_images = [os.path.join(folder1_path, filename), os.path.join(folder3_path, filename), os.path.join(folder2_path, filename)]\n",
        "\n",
        "        # Set titles for each image\n",
        "        titles = ['original image', 'inpinting using original masks', 'inpainting using team #1 masks']\n",
        "\n",
        "        # Plot the images side by side\n",
        "        plot_images_side_by_side(selected_images, titles)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {filename}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HrH0Ny83PV82",
        "mOQ-IrwqannD",
        "VbwhLILVvV-e",
        "TlguIu-fd51G"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
